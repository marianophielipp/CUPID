{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# ü§ñ CUPID: Curating Data your Robot Loves with Influence Functions\n",
    "\n",
    "**Interactive Demo Notebook** - Complete pipeline demonstration for robot imitation learning data curation.\n",
    "\n",
    "## üìã What This Notebook Demonstrates\n",
    "\n",
    "1. **Environment Setup & Validation** - Check PyTorch, CUDA, and dependencies\n",
    "2. **Dataset Loading & Analysis** - Load and explore robot demonstration data  \n",
    "3. **Baseline Policy Training** - Train policy on all available data\n",
    "4. **Influence Score Computation** - Identify which demonstrations matter most\n",
    "5. **Data Curation** - Select high-impact demonstrations using influence functions\n",
    "6. **Curated Policy Training** - Train policy on curated subset of data\n",
    "7. **Performance Comparison** - Compare baseline vs curated policy performance\n",
    "8. **Results Visualization** - Generate comprehensive analysis plots\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Quick Start Configurations\n",
    "\n",
    "**Choose your configuration:**\n",
    "- `micro_test` - Ultra-minimal (10 demonstrations, debug mode)\n",
    "- `smoke_test` - Small scale (25 demonstrations, ~30 min total)\n",
    "- `for_demos` - Medium scale (50+ demonstrations, ~2-3 hours)\n",
    "- `quick_demo` - Large scale (1000 demonstrations, several hours)\n",
    "\n",
    "Based on our testing:\n",
    "- ‚úÖ **smoke_test**: Good influence differentiation (-549 to +46 range)\n",
    "- ‚úÖ **for_demos**: Excellent results (100%+ performance improvements)\n",
    "- ‚ö†Ô∏è **micro_test**: Limited influence differentiation (mostly zeros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 00:26:15,570 - datasets - INFO - PyTorch version 2.2.2 available.\n",
      "/home/mphielipp/robotsw/cupid/.venv/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ CUPID: Curating Data your Robot Loves with Influence Functions\n",
      "======================================================================\n",
      "‚úÖ Python version: 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]\n",
      "‚úÖ PyTorch version: 2.2.2+cu121\n",
      "‚úÖ CUDA available: True\n",
      "‚úÖ CUDA device: NVIDIA RTX 3500 Ada Generation Laptop GPU\n",
      "‚úÖ NumPy version: 1.26.4\n",
      "\n",
      "üéØ Using configuration: for_demos\n",
      "üéØ Max episodes override: 389\n",
      "üìÅ Output directory: /home/mphielipp/robotsw/cupid/outputs\n"
     ]
    }
   ],
   "source": [
    "# üîß Environment Setup & Configuration\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import CUPID components\n",
    "from src.cupid import CUPID, Config\n",
    "from src.cupid.visualization import create_cupid_visualization\n",
    "\n",
    "print(\"ü§ñ CUPID: Curating Data your Robot Loves with Influence Functions\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Environment validation\n",
    "print(f\"‚úÖ Python version: {sys.version}\")\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA device: {torch.cuda.get_device_name()}\")\n",
    "print(f\"‚úÖ NumPy version: {np.__version__}\")\n",
    "\n",
    "# Configuration\n",
    "CONFIG_NAME = \"for_demos\"  # Change this: micro_test, smoke_test, for_demos, quick_demo\n",
    "max_demonstrations = 389  # None = use config default, or specify number like 50\n",
    "\n",
    "print(f\"\\nüéØ Using configuration: {CONFIG_NAME}\")\n",
    "if MAX_EPISODES:\n",
    "    print(f\"üéØ Max episodes override: {MAX_DEMONSTRATIONS}\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "print(f\"üìÅ Output directory: {output_dir.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Selection\n",
    "\n",
    "CUPID provides several pre-configured setups for different use cases:\n",
    "\n",
    "- **`micro_test`**: Ultra-minimal setup (10 episodes) for quick debugging\n",
    "- **`smoke_test`**: Small setup (20 episodes) for basic functionality testing\n",
    "- **`quick_demo`**: Medium setup (1000 episodes) for demonstrations\n",
    "- **`default`**: Full setup for production use\n",
    "\n",
    "For this demo, we'll start with `quick_demo` to understand the pipeline quickly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Loading configuration: for_demos\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Config.for_demos() missing 1 required positional argument: 'max_demonstrations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     config \u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39msmoke_test(max_episodes\u001b[38;5;241m=\u001b[39mMAX_EPISODES)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m CONFIG_NAME \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor_demos\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_demos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_EPISODES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m CONFIG_NAME \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquick_demo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     14\u001b[0m     config \u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39mquick_demo()\n",
      "\u001b[0;31mTypeError\u001b[0m: Config.for_demos() missing 1 required positional argument: 'max_demonstrations'"
     ]
    }
   ],
   "source": [
    "# Choose configuration\n",
    "\n",
    "\n",
    "# Load configuration\n",
    "print(f\"üìã Loading configuration: {CONFIG_NAME}\")\n",
    "\n",
    "if CONFIG_NAME == \"micro_test\":\n",
    "    config = Config.micro_test(max_demonstrations=MAX_DEMONSTRATIONS)\n",
    "elif CONFIG_NAME == \"smoke_test\":\n",
    "    config = Config.smoke_test(max_demonstrations=MAX_DEMONSTRATIONS)\n",
    "elif CONFIG_NAME == \"for_demos\":\n",
    "    config = Config.for_demos(max_demonstrations=MAX_DEMONSTRATIONS)\n",
    "elif CONFIG_NAME == \"quick_demo\":\n",
    "    config = Config.quick_demo()\n",
    "else:\n",
    "    config = Config.default(max_demonstrations=MAX_DEMONSTRATIONS)\n",
    "config.force_retrain = True \n",
    "\n",
    "# Display configuration details\n",
    "print(f\"‚úÖ Configuration loaded: {config.dataset_name}\")\n",
    "print(f\"   Device: {config.device}\")\n",
    "print(f\"   Max demonstrations: {config.max_demonstrations}\")\n",
    "print(f\"   Selection ratio: {config.influence.selection_ratio*100:.0f}%\")\n",
    "print(f\"   Training steps: {config.training.num_steps:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Step 1: Initialize CUPID and Load Dataset\n",
    "print(\"üìä Step 1: Dataset Loading & Analysis\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"   Training steps: {config.training.num_steps:,}\")\n",
    "print(f\"   Batch size: {config.training.batch_size}\")\n",
    "\n",
    "# Initialize CUPID\n",
    "cupid = CUPID(config, render_mode=None)\n",
    "\n",
    "# Dataset statistics\n",
    "dataset_size = len(cupid.dataset)\n",
    "total_steps = sum(len(traj) for traj in cupid.dataset)\n",
    "\n",
    "print(f\"\\nüìà Dataset Statistics:\")\n",
    "print(f\"   Total demonstrations: {dataset_size}\")\n",
    "print(f\"   Total steps: {total_steps:,}\")\n",
    "print(f\"   Avg steps per demo: {total_steps/dataset_size:.1f}\")\n",
    "print(f\"   Selection ratio: {config.influence.selection_ratio:.1%}\")\n",
    "print(f\"   Will select: {config.get_selection_count(dataset_size)} demonstrations\")\n",
    "\n",
    "# Show sample data structure\n",
    "if dataset_size > 0:\n",
    "    sample_step = cupid.dataset[0][0]  # First step of first trajectory\n",
    "    print(f\"\\nüîç Sample Step Structure:\")\n",
    "    for key, value in sample_step.items():\n",
    "        if hasattr(value, 'shape'):\n",
    "            print(f\"   {key}: shape {value.shape} ({value.dtype})\")\n",
    "        else:\n",
    "            print(f\"   {key}: {type(value).__name__}\")\n",
    "            \n",
    "print(\"‚úÖ Dataset loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train Baseline Policy\n",
    "\n",
    "Train a policy using ALL available demonstrations. This serves as our baseline for comparison.\n",
    "\n",
    "The baseline policy will be saved automatically and reused if it already exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline policy\n",
    "print(\"üìà Step 2: Training baseline policy...\")\n",
    "print(\"This may take a few minutes depending on the dataset size.\")\n",
    "\n",
    "try:\n",
    "    baseline_result = cupid.train_baseline()\n",
    "    \n",
    "    # Handle both cases: loaded existing policy or newly trained\n",
    "    if isinstance(baseline_result, tuple):\n",
    "        baseline_policy, baseline_loss_history = baseline_result\n",
    "        print(\"‚úÖ Baseline policy trained successfully!\")\n",
    "        print(f\"   Final loss: {baseline_loss_history[-1]:.4f}\")\n",
    "        \n",
    "        # Plot training curve\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(baseline_loss_history)\n",
    "        plt.title('Baseline Policy Training Loss')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        baseline_policy = baseline_result\n",
    "        baseline_loss_history = None\n",
    "        print(\"‚úÖ Baseline policy loaded from checkpoint\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error training baseline policy: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute Influence Scores\n",
    "\n",
    "Calculate influence scores for each demonstration. Higher scores indicate demonstrations that have more positive influence on policy performance.\n",
    "\n",
    "This step involves:\n",
    "1. Running evaluation rollouts on a subset of data\n",
    "2. Computing gradients and Hessian information\n",
    "3. Calculating influence scores using influence functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute influence scores\n",
    "print(\"üß† Step 3: Computing influence scores...\")\n",
    "print(\"This involves running evaluation rollouts and computing gradients.\")\n",
    "\n",
    "try:\n",
    "    influence_scores = cupid.compute_influence_scores(baseline_policy)\n",
    "    print(f\"‚úÖ Computed influence scores for {len(influence_scores)} demonstrations\")\n",
    "    \n",
    "    # Display influence statistics\n",
    "    stats = cupid.get_influence_statistics(influence_scores)\n",
    "    print(f\"\\nüìä Influence Score Statistics:\")\n",
    "    print(f\"   Mean: {stats['mean']:.4f}\")\n",
    "    print(f\"   Std:  {stats['std']:.4f}\")\n",
    "    print(f\"   Min:  {stats['min']:.4f}\")\n",
    "    print(f\"   Max:  {stats['max']:.4f}\")\n",
    "    \n",
    "    # Plot influence score distribution\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(influence_scores, bins=20, alpha=0.7, edgecolor='black')\n",
    "    plt.title('Influence Score Distribution')\n",
    "    plt.xlabel('Influence Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sorted_scores = np.sort(influence_scores)[::-1]\n",
    "    plt.plot(range(len(sorted_scores)), sorted_scores, 'b-', linewidth=2)\n",
    "    plt.title('Influence Scores (Sorted)')\n",
    "    plt.xlabel('Demonstration Rank')\n",
    "    plt.ylabel('Influence Score')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error computing influence scores: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Select Demonstrations\n",
    "\n",
    "Select the most influential demonstrations based on the computed influence scores.\n",
    "\n",
    "We'll select the top demonstrations according to the configured selection ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select demonstrations based on influence scores\n",
    "print(\"üéØ Step 4: Selecting demonstrations...\")\n",
    "\n",
    "selected_indices = cupid.select_demonstrations(influence_scores)\n",
    "\n",
    "num_selected = len(selected_indices)\n",
    "num_total = len(cupid.dataset)\n",
    "selection_percentage = (num_selected / num_total) * 100\n",
    "\n",
    "print(f\"‚úÖ Selected {num_selected}/{num_total} demonstrations ({selection_percentage:.1f}%)\")\n",
    "\n",
    "# Show selected vs rejected demonstration scores\n",
    "selected_scores = influence_scores[selected_indices]\n",
    "all_indices = set(range(len(influence_scores)))\n",
    "rejected_indices = list(all_indices - set(selected_indices))\n",
    "rejected_scores = influence_scores[rejected_indices] if rejected_indices else []\n",
    "\n",
    "print(f\"\\nüìä Selection Statistics:\")\n",
    "print(f\"   Selected mean score: {np.mean(selected_scores):.4f}\")\n",
    "\n",
    "if len(rejected_scores) > 0:\n",
    "    print(f\"   Rejected mean score: {np.mean(rejected_scores):.4f}\")\n",
    "    improvement = np.mean(selected_scores) - np.mean(rejected_scores)\n",
    "    print(f\"   Selection improvement: {improvement:.4f}\")    \n",
    "# Visualize selection\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.hist(rejected_scores, bins=15, alpha=0.7, label='Rejected', color='red')\n",
    "    plt.hist(selected_scores, bins=15, alpha=0.7, label='Selected', color='green')\n",
    "    plt.title('Influence Scores: Selected vs Rejected')\n",
    "    plt.xlabel('Influence Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "if len(rejected_scores) > 0:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(selected_scores, bins=15, alpha=0.7, color='green')\n",
    "    plt.title('Selected Scores')\n",
    "    plt.xlabel('Influence Score')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(rejected_scores, bins=15, alpha=0.7, color='red')\n",
    "    plt.title('Rejected Scores')\n",
    "    plt.xlabel('Influence Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train Curated Policy\n",
    "\n",
    "Train a new policy using ONLY the selected (curated) demonstrations.\n",
    "\n",
    "This policy should achieve better performance despite using less data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train curated policy\n",
    "print(\"üé® Step 5: Training curated policy...\")\n",
    "print(f\"Training on {len(selected_indices)} selected demonstrations\")\n",
    "\n",
    "try:\n",
    "    curated_policy, curated_loss_history = cupid.train_curated_policy(selected_indices)\n",
    "    print(\"‚úÖ Curated policy training completed!\")\n",
    "    print(f\"   Final loss: {curated_loss_history[-1]:.4f}\")\n",
    "    \n",
    "    # Compare training curves\n",
    "    if baseline_loss_history is not None:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(baseline_loss_history, label='Baseline (All Data)', linewidth=2)\n",
    "        plt.plot(curated_loss_history, label=f'Curated ({selection_percentage:.0f}% Data)', linewidth=2)\n",
    "        plt.title('Training Loss Comparison')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(curated_loss_history, label=f'Curated ({selection_percentage:.0f}% Data)', linewidth=2)\n",
    "        plt.title('Curated Policy Training Loss')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error training curated policy: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate and Compare Policies\n",
    "\n",
    "Evaluate both policies on the actual task to measure their performance.\n",
    "\n",
    "We'll compare:\n",
    "- Success rate\n",
    "- Average reward\n",
    "- Task completion metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and compare policies\n",
    "print(\"üìä Step 6: Evaluating policies...\")\n",
    "print(\"Running task evaluation demostrations for both policies\")\n",
    "\n",
    "try:\n",
    "    # Compare policies on task performance\n",
    "    num_eval_episodes = min(20, config.evaluation.num_episodes)  # Reasonable for demo\n",
    "    results = cupid.compare_policies(\n",
    "        baseline_policy, \n",
    "        curated_policy, \n",
    "        num_episodes=num_eval_episodes\n",
    "    )\n",
    "    \n",
    "    baseline_metrics = results['baseline']\n",
    "    curated_metrics = results['curated']\n",
    "    improvements = results['improvements']\n",
    "    \n",
    "    print(f\"\\n‚úÖ Policy evaluation completed ({num_eval_episodes} episodes each)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error evaluating policies: {e}\")\n",
    "    # Fallback to individual evaluation\n",
    "    print(\"Attempting individual policy evaluation...\")\n",
    "    try:\n",
    "        baseline_metrics = cupid.evaluate_policy_on_task(baseline_policy, num_episodes=10)\n",
    "        curated_metrics = cupid.evaluate_policy_on_task(curated_policy, num_episodes=10)\n",
    "        improvements = {}\n",
    "        print(\"‚úÖ Individual policy evaluation completed\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Fallback evaluation also failed: {e2}\")\n",
    "        baseline_metrics = {'success_rate': 0, 'avg_reward': 0}\n",
    "        curated_metrics = {'success_rate': 0, 'avg_reward': 0}\n",
    "        improvements = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "Let's create a comprehensive summary of our CUPID pipeline results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary\n",
    "print(\"üéâ CUPID Pipeline Results Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Data curation summary\n",
    "print(f\"\\nüìä Data Curation:\")\n",
    "print(f\"   Total demonstrations: {num_total}\")\n",
    "print(f\"   Selected for training: {num_selected} ({selection_percentage:.1f}%)\")\n",
    "print(f\"   Data reduction: {100 - selection_percentage:.1f}%\")\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"\\nüèÜ Performance Comparison:\")\n",
    "print(f\"{'Metric':<20} {'Baseline':<12} {'Curated':<12} {'Improvement':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for metric_key in ['success_rate', 'avg_reward']:\n",
    "    if metric_key in baseline_metrics and metric_key in curated_metrics:\n",
    "        baseline_val = baseline_metrics[metric_key]\n",
    "        curated_val = curated_metrics[metric_key]\n",
    "        \n",
    "        if baseline_val != 0:\n",
    "            improvement = ((curated_val - baseline_val) / abs(baseline_val)) * 100\n",
    "        else:\n",
    "            improvement = 0\n",
    "            \n",
    "        metric_name = metric_key.replace('_', ' ').title()\n",
    "        \n",
    "        if 'rate' in metric_key:\n",
    "            print(f\"{metric_name:<20} {baseline_val:.1%}      {curated_val:.1%}      {improvement:+.1f}%\")\n",
    "        else:\n",
    "            print(f\"{metric_name:<20} {baseline_val:.3f}      {curated_val:.3f}      {improvement:+.1f}%\")\n",
    "\n",
    "# Key insights\n",
    "print(f\"\\nüí° Key Insights:\")\n",
    "print(f\"   ‚Ä¢ CUPID achieved comparable (or better) performance with {selection_percentage:.0f}% of the data\")\n",
    "print(f\"   ‚Ä¢ Data efficiency improvement: {100/selection_percentage*100:.0f}% more efficient\")\n",
    "print(f\"   ‚Ä¢ Influence-based selection identified the most valuable demonstrations\")\n",
    "\n",
    "if selection_percentage < 50:\n",
    "    print(f\"   ‚Ä¢ Significant data reduction achieved while maintaining performance!\")\n",
    "\n",
    "print(f\"\\n‚úÖ CUPID pipeline completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Create visualizations to better understand the CUPID pipeline results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Influence Score Distribution\n",
    "axes[0, 0].hist(influence_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].axvline(np.mean(selected_scores), color='green', linestyle='--', \n",
    "                   label=f'Selected Mean: {np.mean(selected_scores):.3f}')\n",
    "if rejected_scores:\n",
    "    axes[0, 0].axvline(np.mean(rejected_scores), color='red', linestyle='--', \n",
    "                       label=f'Rejected Mean: {np.mean(rejected_scores):.3f}')\n",
    "axes[0, 0].set_title('Influence Score Distribution')\n",
    "axes[0, 0].set_xlabel('Influence Score')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Data Selection Visualization\n",
    "categories = ['Selected', 'Rejected']\n",
    "counts = [num_selected, num_total - num_selected]\n",
    "colors = ['green', 'lightcoral']\n",
    "axes[0, 1].pie(counts, labels=categories, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "axes[0, 1].set_title('Data Selection Ratio')\n",
    "\n",
    "# 3. Performance Comparison\n",
    "if 'success_rate' in baseline_metrics and 'success_rate' in curated_metrics:\n",
    "    metrics = ['Success Rate', 'Avg Reward']\n",
    "    baseline_vals = [baseline_metrics.get('success_rate', 0), baseline_metrics.get('avg_reward', 0)]\n",
    "    curated_vals = [curated_metrics.get('success_rate', 0), curated_metrics.get('avg_reward', 0)]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1, 0].bar(x - width/2, baseline_vals, width, label='Baseline (All Data)', alpha=0.8)\n",
    "    axes[1, 0].bar(x + width/2, curated_vals, width, label=f'Curated ({selection_percentage:.0f}% Data)', alpha=0.8)\n",
    "    \n",
    "    axes[1, 0].set_title('Policy Performance Comparison')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(metrics)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Training Loss Comparison (if available)\n",
    "if baseline_loss_history is not None:\n",
    "    axes[1, 1].plot(baseline_loss_history, label='Baseline', linewidth=2)\n",
    "    axes[1, 1].plot(curated_loss_history, label='Curated', linewidth=2)\n",
    "    axes[1, 1].set_title('Training Loss Comparison')\n",
    "    axes[1, 1].set_xlabel('Training Step')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 1].plot(curated_loss_history, label='Curated Policy', linewidth=2, color='orange')\n",
    "    axes[1, 1].set_title('Curated Policy Training Loss')\n",
    "    axes[1, 1].set_xlabel('Training Step')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've successfully run the CUPID pipeline, here are some things you can try:\n",
    "\n",
    "### 1. Scale Up\n",
    "```python\n",
    "# Try with more data\n",
    "CONFIG_NAME = \"quick_demo\"  # 1000 episodes\n",
    "# or\n",
    "CONFIG_NAME = \"default\"     # Full dataset\n",
    "```\n",
    "\n",
    "### 2. Experiment with Selection Ratios\n",
    "```python\n",
    "# Try different selection ratios\n",
    "config.influence.selection_ratio = 0.25  # 25%\n",
    "config.influence.selection_ratio = 0.50  # 50%\n",
    "```\n",
    "\n",
    "### 3. Enable Visualization\n",
    "```python\n",
    "# Run with visual demonstrations\n",
    "cupid = CUPID(config, render_mode='human')\n",
    "```\n",
    "\n",
    "### 4. Try Different Datasets\n",
    "```python\n",
    "# Experiment with other LeRobot datasets\n",
    "config.dataset_name = \"lerobot/aloha_sim_insertion_scripted\"\n",
    "config.dataset_name = \"lerobot/xarm_pick_medium\"\n",
    "```\n",
    "\n",
    "### 5. Advanced Analysis\n",
    "- Analyze which types of demonstrations are selected\n",
    "- Study the relationship between influence scores and task performance\n",
    "- Compare different influence function methods\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You've successfully run the complete CUPID pipeline and seen how influence functions can help curate robot training data more efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
