{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ü§ñ CUPID: Curating Data your Robot Loves with Influence Functions\n",
        "\n",
        "**Interactive Demo Notebook** - Complete pipeline demonstration for robot imitation learning data curation.\n",
        "\n",
        "## üìã What This Notebook Demonstrates\n",
        "\n",
        "1. **Environment Setup & Validation** - Check PyTorch, CUDA, and dependencies\n",
        "2. **Dataset Loading & Analysis** - Load and explore robot demonstration data  \n",
        "3. **Baseline Policy Training** - Train policy on all available data\n",
        "4. **Influence Score Computation** - Identify which demonstrations matter most\n",
        "5. **Data Curation** - Select high-impact demonstrations using influence functions\n",
        "6. **Curated Policy Training** - Train policy on curated subset of data\n",
        "7. **Performance Comparison** - Compare baseline vs curated policy performance\n",
        "8. **Results Visualization** - Generate comprehensive analysis plots\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Quick Start Configurations\n",
        "\n",
        "**Choose your configuration:**\n",
        "- `micro_test` - Ultra-minimal (10 episodes, debug mode)\n",
        "- `smoke_test` - Small scale (25 episodes, ~30 min total)\n",
        "- `for_demos` - Medium scale (50+ episodes, ~2-3 hours)\n",
        "- `quick_demo` - Large scale (1000 episodes, several hours)\n",
        "\n",
        "Based on our testing:\n",
        "- ‚úÖ **smoke_test**: Good influence differentiation (-549 to +46 range)\n",
        "- ‚úÖ **for_demos**: Excellent results (100%+ performance improvements)\n",
        "- ‚ö†Ô∏è **micro_test**: Limited influence differentiation (mostly zeros)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß Environment Setup & Configuration\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Import CUPID components\n",
        "from src.cupid import CUPID, Config\n",
        "from src.cupid.visualization import create_cupid_visualization\n",
        "\n",
        "print(\"ü§ñ CUPID: Curating Data your Robot Loves with Influence Functions\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Environment validation\n",
        "print(f\"‚úÖ Python version: {sys.version}\")\n",
        "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
        "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ CUDA device: {torch.cuda.get_device_name()}\")\n",
        "print(f\"‚úÖ NumPy version: {np.__version__}\")\n",
        "\n",
        "# Configuration\n",
        "CONFIG_NAME = \"smoke_test\"  # Change this: micro_test, smoke_test, for_demos, quick_demo\n",
        "MAX_EPISODES = None  # None = use config default, or specify number like 50\n",
        "\n",
        "print(f\"\\nüéØ Using configuration: {CONFIG_NAME}\")\n",
        "if MAX_EPISODES:\n",
        "    print(f\"üéØ Max episodes override: {MAX_EPISODES}\")\n",
        "\n",
        "# Create output directory\n",
        "output_dir = Path(\"outputs\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "print(f\"üìÅ Output directory: {output_dir.absolute()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Step 1: Initialize CUPID and Load Dataset\n",
        "print(\"üìä Step 1: Dataset Loading & Analysis\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Initialize configuration\n",
        "if CONFIG_NAME == \"micro_test\":\n",
        "    config = Config.micro_test()\n",
        "elif CONFIG_NAME == \"smoke_test\":\n",
        "    config = Config.smoke_test()\n",
        "elif CONFIG_NAME == \"for_demos\":\n",
        "    config = Config.for_demos(max_episodes=MAX_EPISODES)\n",
        "elif CONFIG_NAME == \"quick_demo\":\n",
        "    config = Config.quick_demo()\n",
        "else:\n",
        "    config = Config.default()\n",
        "\n",
        "print(f\"‚úÖ Configuration: {config.dataset_name}\")\n",
        "print(f\"   Device: {config.device}\")\n",
        "print(f\"   Training steps: {config.training.num_steps:,}\")\n",
        "print(f\"   Batch size: {config.training.batch_size}\")\n",
        "\n",
        "# Initialize CUPID\n",
        "cupid = CUPID(config, render_mode=None)\n",
        "\n",
        "# Dataset statistics\n",
        "dataset_size = len(cupid.dataset)\n",
        "total_steps = sum(len(traj) for traj in cupid.dataset)\n",
        "\n",
        "print(f\"\\nüìà Dataset Statistics:\")\n",
        "print(f\"   Total demonstrations: {dataset_size}\")\n",
        "print(f\"   Total steps: {total_steps:,}\")\n",
        "print(f\"   Avg steps per demo: {total_steps/dataset_size:.1f}\")\n",
        "print(f\"   Selection ratio: {config.influence.selection_ratio:.1%}\")\n",
        "print(f\"   Will select: {config.get_selection_count(dataset_size)} demonstrations\")\n",
        "\n",
        "# Show sample data structure\n",
        "if dataset_size > 0:\n",
        "    sample_step = cupid.dataset[0][0]  # First step of first trajectory\n",
        "    print(f\"\\nüîç Sample Step Structure:\")\n",
        "    for key, value in sample_step.items():\n",
        "        if hasattr(value, 'shape'):\n",
        "            print(f\"   {key}: shape {value.shape} ({value.dtype})\")\n",
        "        else:\n",
        "            print(f\"   {key}: {type(value).__name__}\")\n",
        "            \n",
        "print(\"‚úÖ Dataset loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# CUPID: Curating Data your Robot Loves with Influence Functions\n",
        "\n",
        "This notebook demonstrates the complete CUPID pipeline for robot imitation learning data curation.\n",
        "\n",
        "**Paper**: CUPID: Curating Data your Robot Loves with Influence Functions  \n",
        "**Key Result**: Training with ~25-33% of curated data can achieve state-of-the-art performance\n",
        "\n",
        "## Overview\n",
        "\n",
        "CUPID uses influence functions to identify the most valuable demonstrations for training robot policies. The pipeline consists of:\n",
        "\n",
        "1. **Baseline Training**: Train a policy on all available demonstrations\n",
        "2. **Influence Computation**: Calculate how much each demonstration influences policy performance\n",
        "3. **Data Curation**: Select the most influential demonstrations\n",
        "4. **Curated Training**: Train a new policy on only the selected data\n",
        "5. **Evaluation**: Compare baseline vs curated policy performance\n",
        "\n",
        "Let's walk through each step!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## Setup and Imports\n",
        "\n",
        "First, let's set up our environment and import the necessary modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# Configure logging for clear output\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Add src to path for imports\n",
        "sys.path.append(str(Path.cwd() / \"src\"))\n",
        "\n",
        "# Import CUPID modules\n",
        "try:\n",
        "    from cupid import CUPID, Config\n",
        "    from cupid.visualization import create_cupid_visualization\n",
        "    print(\"‚úÖ CUPID modules imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import CUPID modules: {e}\")\n",
        "    print(\"Make sure you're running from the project root\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Configuration Selection\n",
        "\n",
        "CUPID provides several pre-configured setups for different use cases:\n",
        "\n",
        "- **`micro_test`**: Ultra-minimal setup (10 episodes) for quick debugging\n",
        "- **`smoke_test`**: Small setup (20 episodes) for basic functionality testing\n",
        "- **`quick_demo`**: Medium setup (1000 episodes) for demonstrations\n",
        "- **`default`**: Full setup for production use\n",
        "\n",
        "For this demo, we'll start with `micro_test` to understand the pipeline quickly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose configuration\n",
        "CONFIG_NAME = \"micro_test\"  # Change to \"quick_demo\" for larger dataset\n",
        "MAX_EPISODES = 10  # Small for demo purposes\n",
        "\n",
        "# Load configuration\n",
        "print(f\"üìã Loading configuration: {CONFIG_NAME}\")\n",
        "\n",
        "if CONFIG_NAME == \"micro_test\":\n",
        "    config = Config.micro_test(max_episodes=MAX_EPISODES)\n",
        "elif CONFIG_NAME == \"smoke_test\":\n",
        "    config = Config.smoke_test(max_episodes=MAX_EPISODES)\n",
        "elif CONFIG_NAME == \"quick_demo\":\n",
        "    config = Config.quick_demo()\n",
        "else:\n",
        "    config = Config.default(max_episodes=MAX_EPISODES)\n",
        "\n",
        "# Display configuration details\n",
        "print(f\"‚úÖ Configuration loaded: {config.dataset_name}\")\n",
        "print(f\"   Device: {config.device}\")\n",
        "print(f\"   Max episodes: {config.max_episodes}\")\n",
        "print(f\"   Selection ratio: {config.influence.selection_ratio*100:.0f}%\")\n",
        "print(f\"   Training steps: {config.training.num_steps:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 1: Initialize CUPID\n",
        "\n",
        "Create the CUPID instance and load the dataset. This will:\n",
        "- Load robot demonstrations from the specified dataset\n",
        "- Initialize all pipeline components\n",
        "- Set up the environment for evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize CUPID\n",
        "print(\"ü§ñ Initializing CUPID...\")\n",
        "cupid = CUPID(config, render_mode=None)  # No rendering for notebook\n",
        "\n",
        "# Display dataset information\n",
        "print(f\"üìä Dataset loaded: {config.dataset_name}\")\n",
        "print(f\"üéØ Total demonstrations: {len(cupid.dataset)}\")\n",
        "print(f\"üéØ Selection ratio: {config.influence.selection_ratio*100:.0f}%\")\n",
        "\n",
        "# Show sample trajectory structure\n",
        "if cupid.dataset:\n",
        "    sample_traj = cupid.dataset[0]\n",
        "    print(f\"\\nüìã Sample trajectory structure:\")\n",
        "    print(f\"   Length: {len(sample_traj)} steps\")\n",
        "    if sample_traj:\n",
        "        sample_step = sample_traj[0]\n",
        "        print(f\"   Keys: {list(sample_step.keys())}\")\n",
        "        if 'observation.state' in sample_step:\n",
        "            state_shape = np.array(sample_step['observation.state']).shape\n",
        "            print(f\"   State shape: {state_shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 2: Train Baseline Policy\n",
        "\n",
        "Train a policy using ALL available demonstrations. This serves as our baseline for comparison.\n",
        "\n",
        "The baseline policy will be saved automatically and reused if it already exists.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train baseline policy\n",
        "print(\"üìà Step 2: Training baseline policy...\")\n",
        "print(\"This may take a few minutes depending on the dataset size.\")\n",
        "\n",
        "try:\n",
        "    baseline_result = cupid.train_baseline()\n",
        "    \n",
        "    # Handle both cases: loaded existing policy or newly trained\n",
        "    if isinstance(baseline_result, tuple):\n",
        "        baseline_policy, baseline_loss_history = baseline_result\n",
        "        print(\"‚úÖ Baseline policy trained successfully!\")\n",
        "        print(f\"   Final loss: {baseline_loss_history[-1]:.4f}\")\n",
        "        \n",
        "        # Plot training curve\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(baseline_loss_history)\n",
        "        plt.title('Baseline Policy Training Loss')\n",
        "        plt.xlabel('Step')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    else:\n",
        "        baseline_policy = baseline_result\n",
        "        baseline_loss_history = None\n",
        "        print(\"‚úÖ Baseline policy loaded from checkpoint\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error training baseline policy: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 3: Compute Influence Scores\n",
        "\n",
        "Calculate influence scores for each demonstration. Higher scores indicate demonstrations that have more positive influence on policy performance.\n",
        "\n",
        "This step involves:\n",
        "1. Running evaluation rollouts on a subset of data\n",
        "2. Computing gradients and Hessian information\n",
        "3. Calculating influence scores using influence functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute influence scores\n",
        "print(\"üß† Step 3: Computing influence scores...\")\n",
        "print(\"This involves running evaluation rollouts and computing gradients.\")\n",
        "\n",
        "try:\n",
        "    influence_scores = cupid.compute_influence_scores(baseline_policy)\n",
        "    print(f\"‚úÖ Computed influence scores for {len(influence_scores)} demonstrations\")\n",
        "    \n",
        "    # Display influence statistics\n",
        "    stats = cupid.get_influence_statistics(influence_scores)\n",
        "    print(f\"\\nüìä Influence Score Statistics:\")\n",
        "    print(f\"   Mean: {stats['mean']:.4f}\")\n",
        "    print(f\"   Std:  {stats['std']:.4f}\")\n",
        "    print(f\"   Min:  {stats['min']:.4f}\")\n",
        "    print(f\"   Max:  {stats['max']:.4f}\")\n",
        "    \n",
        "    # Plot influence score distribution\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(influence_scores, bins=20, alpha=0.7, edgecolor='black')\n",
        "    plt.title('Influence Score Distribution')\n",
        "    plt.xlabel('Influence Score')\n",
        "    plt.ylabel('Count')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    sorted_scores = np.sort(influence_scores)[::-1]\n",
        "    plt.plot(range(len(sorted_scores)), sorted_scores, 'b-', linewidth=2)\n",
        "    plt.title('Influence Scores (Sorted)')\n",
        "    plt.xlabel('Demonstration Rank')\n",
        "    plt.ylabel('Influence Score')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error computing influence scores: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 4: Select Demonstrations\n",
        "\n",
        "Select the most influential demonstrations based on the computed influence scores.\n",
        "\n",
        "We'll select the top demonstrations according to the configured selection ratio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select demonstrations based on influence scores\n",
        "print(\"üéØ Step 4: Selecting demonstrations...\")\n",
        "\n",
        "selected_indices = cupid.select_demonstrations(influence_scores)\n",
        "\n",
        "num_selected = len(selected_indices)\n",
        "num_total = len(cupid.dataset)\n",
        "selection_percentage = (num_selected / num_total) * 100\n",
        "\n",
        "print(f\"‚úÖ Selected {num_selected}/{num_total} demonstrations ({selection_percentage:.1f}%)\")\n",
        "\n",
        "# Show selected vs rejected demonstration scores\n",
        "selected_scores = influence_scores[selected_indices]\n",
        "all_indices = set(range(len(influence_scores)))\n",
        "rejected_indices = list(all_indices - set(selected_indices))\n",
        "rejected_scores = influence_scores[rejected_indices] if rejected_indices else []\n",
        "\n",
        "print(f\"\\nüìä Selection Statistics:\")\n",
        "print(f\"   Selected mean score: {np.mean(selected_scores):.4f}\")\n",
        "if rejected_scores:\n",
        "    print(f\"   Rejected mean score: {np.mean(rejected_scores):.4f}\")\n",
        "    improvement = np.mean(selected_scores) - np.mean(rejected_scores)\n",
        "    print(f\"   Selection improvement: {improvement:.4f}\")\n",
        "\n",
        "# Visualize selection\n",
        "if rejected_scores:\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.hist(rejected_scores, bins=15, alpha=0.7, label='Rejected', color='red')\n",
        "    plt.hist(selected_scores, bins=15, alpha=0.7, label='Selected', color='green')\n",
        "    plt.title('Influence Scores: Selected vs Rejected')\n",
        "    plt.xlabel('Influence Score')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 5: Train Curated Policy\n",
        "\n",
        "Train a new policy using ONLY the selected (curated) demonstrations.\n",
        "\n",
        "This policy should achieve better performance despite using less data!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train curated policy\n",
        "print(\"üé® Step 5: Training curated policy...\")\n",
        "print(f\"Training on {len(selected_indices)} selected demonstrations\")\n",
        "\n",
        "try:\n",
        "    curated_policy, curated_loss_history = cupid.train_curated_policy(selected_indices)\n",
        "    print(\"‚úÖ Curated policy training completed!\")\n",
        "    print(f\"   Final loss: {curated_loss_history[-1]:.4f}\")\n",
        "    \n",
        "    # Compare training curves\n",
        "    if baseline_loss_history is not None:\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(baseline_loss_history, label='Baseline (All Data)', linewidth=2)\n",
        "        plt.plot(curated_loss_history, label=f'Curated ({selection_percentage:.0f}% Data)', linewidth=2)\n",
        "        plt.title('Training Loss Comparison')\n",
        "        plt.xlabel('Step')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(curated_loss_history, label=f'Curated ({selection_percentage:.0f}% Data)', linewidth=2)\n",
        "        plt.title('Curated Policy Training Loss')\n",
        "        plt.xlabel('Step')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error training curated policy: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 6: Evaluate and Compare Policies\n",
        "\n",
        "Evaluate both policies on the actual task to measure their performance.\n",
        "\n",
        "We'll compare:\n",
        "- Success rate\n",
        "- Average reward\n",
        "- Task completion metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate and compare policies\n",
        "print(\"üìä Step 6: Evaluating policies...\")\n",
        "print(\"Running task evaluation episodes for both policies\")\n",
        "\n",
        "try:\n",
        "    # Compare policies on task performance\n",
        "    num_eval_episodes = min(20, config.evaluation.num_episodes)  # Reasonable for demo\n",
        "    results = cupid.compare_policies(\n",
        "        baseline_policy, \n",
        "        curated_policy, \n",
        "        cupid.dataset, \n",
        "        num_episodes=num_eval_episodes\n",
        "    )\n",
        "    \n",
        "    baseline_metrics = results['baseline']\n",
        "    curated_metrics = results['curated']\n",
        "    improvements = results['improvements']\n",
        "    \n",
        "    print(f\"\\n‚úÖ Policy evaluation completed ({num_eval_episodes} episodes each)\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error evaluating policies: {e}\")\n",
        "    # Fallback to individual evaluation\n",
        "    print(\"Attempting individual policy evaluation...\")\n",
        "    try:\n",
        "        baseline_metrics = cupid.evaluate_policy_on_task(baseline_policy, num_episodes=10)\n",
        "        curated_metrics = cupid.evaluate_policy_on_task(curated_policy, num_episodes=10)\n",
        "        improvements = {}\n",
        "        print(\"‚úÖ Individual policy evaluation completed\")\n",
        "    except Exception as e2:\n",
        "        print(f\"‚ùå Fallback evaluation also failed: {e2}\")\n",
        "        baseline_metrics = {'success_rate': 0, 'avg_reward': 0}\n",
        "        curated_metrics = {'success_rate': 0, 'avg_reward': 0}\n",
        "        improvements = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Results Summary\n",
        "\n",
        "Let's create a comprehensive summary of our CUPID pipeline results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create results summary\n",
        "print(\"üéâ CUPID Pipeline Results Summary\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Data curation summary\n",
        "print(f\"\\nüìä Data Curation:\")\n",
        "print(f\"   Total demonstrations: {num_total}\")\n",
        "print(f\"   Selected for training: {num_selected} ({selection_percentage:.1f}%)\")\n",
        "print(f\"   Data reduction: {100 - selection_percentage:.1f}%\")\n",
        "\n",
        "# Performance comparison\n",
        "print(f\"\\nüèÜ Performance Comparison:\")\n",
        "print(f\"{'Metric':<20} {'Baseline':<12} {'Curated':<12} {'Improvement':<12}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for metric_key in ['success_rate', 'avg_reward']:\n",
        "    if metric_key in baseline_metrics and metric_key in curated_metrics:\n",
        "        baseline_val = baseline_metrics[metric_key]\n",
        "        curated_val = curated_metrics[metric_key]\n",
        "        \n",
        "        if baseline_val != 0:\n",
        "            improvement = ((curated_val - baseline_val) / abs(baseline_val)) * 100\n",
        "        else:\n",
        "            improvement = 0\n",
        "            \n",
        "        metric_name = metric_key.replace('_', ' ').title()\n",
        "        \n",
        "        if 'rate' in metric_key:\n",
        "            print(f\"{metric_name:<20} {baseline_val:.1%}      {curated_val:.1%}      {improvement:+.1f}%\")\n",
        "        else:\n",
        "            print(f\"{metric_name:<20} {baseline_val:.3f}      {curated_val:.3f}      {improvement:+.1f}%\")\n",
        "\n",
        "# Key insights\n",
        "print(f\"\\nüí° Key Insights:\")\n",
        "print(f\"   ‚Ä¢ CUPID achieved comparable (or better) performance with {selection_percentage:.0f}% of the data\")\n",
        "print(f\"   ‚Ä¢ Data efficiency improvement: {100/selection_percentage*100:.0f}% more efficient\")\n",
        "print(f\"   ‚Ä¢ Influence-based selection identified the most valuable demonstrations\")\n",
        "\n",
        "if selection_percentage < 50:\n",
        "    print(f\"   ‚Ä¢ Significant data reduction achieved while maintaining performance!\")\n",
        "\n",
        "print(f\"\\n‚úÖ CUPID pipeline completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Visualization\n",
        "\n",
        "Create visualizations to better understand the CUPID pipeline results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# 1. Influence Score Distribution\n",
        "axes[0, 0].hist(influence_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[0, 0].axvline(np.mean(selected_scores), color='green', linestyle='--', \n",
        "                   label=f'Selected Mean: {np.mean(selected_scores):.3f}')\n",
        "if rejected_scores:\n",
        "    axes[0, 0].axvline(np.mean(rejected_scores), color='red', linestyle='--', \n",
        "                       label=f'Rejected Mean: {np.mean(rejected_scores):.3f}')\n",
        "axes[0, 0].set_title('Influence Score Distribution')\n",
        "axes[0, 0].set_xlabel('Influence Score')\n",
        "axes[0, 0].set_ylabel('Count')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Data Selection Visualization\n",
        "categories = ['Selected', 'Rejected']\n",
        "counts = [num_selected, num_total - num_selected]\n",
        "colors = ['green', 'lightcoral']\n",
        "axes[0, 1].pie(counts, labels=categories, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "axes[0, 1].set_title('Data Selection Ratio')\n",
        "\n",
        "# 3. Performance Comparison\n",
        "if 'success_rate' in baseline_metrics and 'success_rate' in curated_metrics:\n",
        "    metrics = ['Success Rate', 'Avg Reward']\n",
        "    baseline_vals = [baseline_metrics.get('success_rate', 0), baseline_metrics.get('avg_reward', 0)]\n",
        "    curated_vals = [curated_metrics.get('success_rate', 0), curated_metrics.get('avg_reward', 0)]\n",
        "    \n",
        "    x = np.arange(len(metrics))\n",
        "    width = 0.35\n",
        "    \n",
        "    axes[1, 0].bar(x - width/2, baseline_vals, width, label='Baseline (All Data)', alpha=0.8)\n",
        "    axes[1, 0].bar(x + width/2, curated_vals, width, label=f'Curated ({selection_percentage:.0f}% Data)', alpha=0.8)\n",
        "    \n",
        "    axes[1, 0].set_title('Policy Performance Comparison')\n",
        "    axes[1, 0].set_xticks(x)\n",
        "    axes[1, 0].set_xticklabels(metrics)\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Training Loss Comparison (if available)\n",
        "if baseline_loss_history is not None:\n",
        "    axes[1, 1].plot(baseline_loss_history, label='Baseline', linewidth=2)\n",
        "    axes[1, 1].plot(curated_loss_history, label='Curated', linewidth=2)\n",
        "    axes[1, 1].set_title('Training Loss Comparison')\n",
        "    axes[1, 1].set_xlabel('Training Step')\n",
        "    axes[1, 1].set_ylabel('Loss')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "else:\n",
        "    axes[1, 1].plot(curated_loss_history, label='Curated Policy', linewidth=2, color='orange')\n",
        "    axes[1, 1].set_title('Curated Policy Training Loss')\n",
        "    axes[1, 1].set_xlabel('Training Step')\n",
        "    axes[1, 1].set_ylabel('Loss')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that you've successfully run the CUPID pipeline, here are some things you can try:\n",
        "\n",
        "### 1. Scale Up\n",
        "```python\n",
        "# Try with more data\n",
        "CONFIG_NAME = \"quick_demo\"  # 1000 episodes\n",
        "# or\n",
        "CONFIG_NAME = \"default\"     # Full dataset\n",
        "```\n",
        "\n",
        "### 2. Experiment with Selection Ratios\n",
        "```python\n",
        "# Try different selection ratios\n",
        "config.influence.selection_ratio = 0.25  # 25%\n",
        "config.influence.selection_ratio = 0.50  # 50%\n",
        "```\n",
        "\n",
        "### 3. Enable Visualization\n",
        "```python\n",
        "# Run with visual demonstrations\n",
        "cupid = CUPID(config, render_mode='human')\n",
        "```\n",
        "\n",
        "### 4. Try Different Datasets\n",
        "```python\n",
        "# Experiment with other LeRobot datasets\n",
        "config.dataset_name = \"lerobot/aloha_sim_insertion_scripted\"\n",
        "config.dataset_name = \"lerobot/xarm_pick_medium\"\n",
        "```\n",
        "\n",
        "### 5. Advanced Analysis\n",
        "- Analyze which types of demonstrations are selected\n",
        "- Study the relationship between influence scores and task performance\n",
        "- Compare different influence function methods\n",
        "\n",
        "---\n",
        "\n",
        "**üéâ Congratulations!** You've successfully run the complete CUPID pipeline and seen how influence functions can help curate robot training data more efficiently.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
